{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "015fa618-570c-43c4-bc1d-4cd55ce9a9a1",
   "metadata": {},
   "source": [
    "# So sánh các phương pháp xây dựng tokenizer cho bài toán phân loại domain trên tập UIT-ViOCD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ecdae8-9f34-45b3-949c-32103e3f7de9",
   "metadata": {},
   "source": [
    "| Phương pháp tokenizer                                     | Mô tả                                                                                                                       | Ưu điểm                                                                                                                                  | Hạn chế                                                                                                                                                             | Mức độ phù hợp với UIT-ViOCD                                                                                          |\n",
    "| --------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Tokenizer theo khoảng trắng (Whitespace-based)**        | Tách câu thành các token dựa trên dấu cách, mỗi từ được xem là một đơn vị độc lập.                                          | Triển khai đơn giản, dễ kiểm soát; phù hợp làm baseline hoặc minh họa nguyên lý cơ bản.                                                  | Không xử lý tốt lỗi chính tả, từ không dấu, teencode, emoji; từ vựng phình to; tỷ lệ OOV cao; không phản ánh cấu trúc hình thái tiếng Việt.                         | **Thấp** – chỉ phù hợp làm mô hình đối chứng (baseline), không phù hợp cho huấn luyện Transformer hiệu quả.           |\n",
    "| **Tokenizer BPE tự huấn luyện trên tập dữ liệu**          | Huấn luyện Byte-Pair Encoding hoặc SentencePiece trực tiếp trên tập train của UIT-ViOCD để sinh vocab đặc thù miền dữ liệu. | Có khả năng thích nghi với dữ liệu mục tiêu; giảm OOV so với whitespace; linh hoạt về kích thước vocab.                                  | Dữ liệu huấn luyện nhỏ (~5.5k câu) dẫn đến subword kém ổn định; tokenizer dễ học theo nhiễu (lỗi chính tả, câu ngắn); chi phí triển khai cao nhưng lợi ích hạn chế. | **Trung bình** – chỉ phù hợp khi có yêu cầu bắt buộc về tokenizer tự xây dựng hoặc mục tiêu nghiên cứu tokenizer.     |\n",
    "| **Tokenizer pretrained (SentencePiece / Byte-level BPE)** | Sử dụng tokenizer đã được huấn luyện trước trên corpora lớn (ví dụ PhoBERT, XLM-R), sau đó áp dụng cho dữ liệu UIT-ViOCD.   | Subword ổn định, giàu thông tin ngôn ngữ; xử lý tốt dữ liệu nhiễu, không dấu, emoji; không OOV; giúp mô hình học nhanh và tổng quát tốt. | Không chuyên biệt tuyệt đối cho miền dữ liệu; phụ thuộc vào tokenizer có sẵn.                                                                                       | **Cao** – lựa chọn phù hợp nhất cho bài toán phân loại domain với Transformer Encoder trong bối cảnh dữ liệu hạn chế. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a850d-1733-4910-b93e-6a58ab87b00a",
   "metadata": {},
   "source": [
    "# 1. Đọc dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e7e421-1202-4565-a295-7c0d735ba89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "data_dir = \"./dataset/UIT-Vi-OCD\"\n",
    "\n",
    "# Đọc train/dev/test small subset data\n",
    "train_data = json.load(open(os.path.join(data_dir, 'train.json'), 'r', encoding='utf-8'))\n",
    "dev_data = json.load(open(os.path.join(data_dir, 'dev.json'), 'r', encoding='utf-8'))\n",
    "test_data = json.load(open(os.path.join(data_dir, 'test.json'), 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9882b29e-967c-4ecc-b847-96a9ae4e04cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4,387 mẫu\n",
      "Dev: 548 mẫu\n",
      "Test: 549 mẫu\n",
      "\n",
      "Sample train:\n",
      "{'review': 'gói hàng cẩn thận . chơi pubg với liên quân mượt với giá như này thì quá tốt', 'label': 'non-complaint', 'domain': 'mobile'}\n",
      "{'review': 'mình góp ý thật nhé . . đừng bắt phải đăng nhập zalo nữa mình muốn tải nhạc mà không được lấy lại tôi không zalo thì gửi tin nhắn mất tiền được mã kích hoạt lại không chính xác . . mình thấy 10 ngưòi thì họ khó chịu cả 10 . . lập trình viên viết ra ứng dụng ngày càng dễ sử dụng đằng này càng ngày lại càng phức tạp và . . . bài đánh giá đầy đủ', 'label': 'complaint', 'domain': 'app'}\n",
      "{'review': 'máy khá đẹp , pin trâu vân tay nhạy nhận diện khuôn mặt nhanh nói chung ổn . tuy chơi game fre fire bị chậm khung hình không mượt lắm nhưng với giá giẫm ngày 1111 được ad mã giảm giá 200k còn hơn 2tr6 thì vậy là ngon rồi', 'label': 'complaint', 'domain': 'mobile'}\n"
     ]
    }
   ],
   "source": [
    "def summarize_split(data, name):\n",
    "    print(f\"{name}: {len(data):,} mẫu\")\n",
    "\n",
    "summarize_split(train_data, \"Train\")\n",
    "summarize_split(dev_data, \"Dev\")\n",
    "summarize_split(test_data, \"Test\")\n",
    "\n",
    "print(\"\\nSample train:\")\n",
    "print(train_data[\"0\"])\n",
    "print(train_data[\"1\"])\n",
    "print(train_data[\"2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a144c8-c7e6-4ae8-bd7d-b6dd1da80d1a",
   "metadata": {},
   "source": [
    "# 2. Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ae5ee-562a-41c6-aac1-315e960b67c5",
   "metadata": {},
   "source": [
    "## 2.1 Lấy text trong key `review`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015fbc55-3c3e-4b8c-9177-e1b90751b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4387\n",
      "Dev:   548\n",
      "Test:  549\n",
      "\n",
      "Sample text:\n",
      "gói hàng cẩn thận . chơi pubg với liên quân mượt với giá như này thì quá tốt\n"
     ]
    }
   ],
   "source": [
    "def collect_texts(data_dict):\n",
    "    return [v[\"review\"] for v in data_dict.values()]\n",
    "\n",
    "train_texts = collect_texts(train_data)\n",
    "dev_texts   = collect_texts(dev_data)\n",
    "test_texts  = collect_texts(test_data)\n",
    "\n",
    "print(f\"Train: {len(train_texts)}\")\n",
    "print(f\"Dev:   {len(dev_texts)}\")\n",
    "print(f\"Test:  {len(test_texts)}\")\n",
    "\n",
    "print(\"\\nSample text:\")\n",
    "print(train_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25599e79-ec67-43fb-b2c1-033ef601e7ea",
   "metadata": {},
   "source": [
    "## 2.2 Chuẩn hóa text tối thiểu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260be25-4e0d-4c00-b418-87504251bf54",
   "metadata": {},
   "source": [
    "Lưu ý: Tokenizer pretrained đã khá tốt, nên ta chỉ preprocessing nhẹ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4330ce1d-c951-40ad-836e-3dd825860ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_text_nfc(text):\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return text\n",
    "\n",
    "train_texts_nfc = [normalize_text_nfc(t) for t in train_texts]\n",
    "dev_texts_nfc   = [normalize_text_nfc(t) for t in dev_texts]\n",
    "test_texts_nfc  = [normalize_text_nfc(t) for t in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80e8abee-5930-4913-a9e9-17c82ddcf107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gói hàng cẩn thận . chơi pubg với liên quân mượt với giá như này thì quá tốt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_nfc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57489a7c-ce96-4a62-8727-59d5666f3831",
   "metadata": {},
   "source": [
    "## 2.3 Load tokenizer pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ea3cd-39ed-4ea9-a642-a0d90cd76f21",
   "metadata": {},
   "source": [
    "## a. XLM Roberta Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa3d7b-e376-404c-b9ed-04fe801ee6e0",
   "metadata": {},
   "source": [
    "Ưu điểm: Được train trên đa ngôn ngữ, đa domain (nhiều lĩnh vực, đã đọc nhiều từ sai chính tả, viết tắt,...), không xảy ra hiện tượng OOV (vì dùng Byte-level Byte Pair Encoding, mã hóa về byte hết nên không có trường hợp tokenizer không hiểu từ).\n",
    "\n",
    "Khuyết điểm: Có thể tạo ra các từ vựng không có thật (subword) tuy không tốt về mặt ngôn ngữ nhưng ổn định về mặt biểu diễn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cced2934-cdb3-4cd0-8dae-852749bab1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2026.1.15-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\app\\anaconda\\envs\\test\\lib\\site-packages (from requests->transformers) (2026.1.4)\n",
      "Downloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.0 MB 7.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.6/12.0 MB 13.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.2/12.0 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 12.4 MB/s  0:00:01\n",
      "Downloading regex-2026.1.15-cp313-cp313-win_amd64.whl (277 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Installing collected packages: safetensors, regex, transformers\n",
      "\n",
      "   ------------- -------------------------- 1/3 [regex]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   ---------------------------------------- 3/3 [transformers]\n",
      "\n",
      "Successfully installed regex-2026.1.15 safetensors-0.7.0 transformers-4.57.6\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ae5064d-9e10-405f-9248-24ea0efd0c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\app\\Anaconda\\envs\\test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_NAME = \"xlm-roberta-base\"\n",
    "xlm_tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "print(xlm_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a2581-eafc-421e-969c-3ab94cec9a18",
   "metadata": {},
   "source": [
    "## b. PhoBert Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f902c4-2b81-4cfc-83db-cf5c491bf87f",
   "metadata": {},
   "source": [
    "Ưu điểm: Tối ưu cho tiếng Việt chuẩn\n",
    "\n",
    "Nhược điểm: được train trên wikipedia và các nguồn văn bản chuẩn, không phù hợp cho domain review (nhiều tên code, viết tắt, sai chính tả), có thể tạo ra OOV (do dùng Sentence với BPE với đơn vị khởi tạo là unicode chứ không phải byte-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8988ad-dd4b-401e-a094-cc238dbaa682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhobertTokenizer(name_or_path='vinai/phobert-base', vocab_size=64000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t64000: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_NAME = \"vinai/phobert-base\"\n",
    "\n",
    "phobert_tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "print(phobert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b7a0470-eac4-44c8-a748-d8e19e30d2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "gói hàng cẩn thận . chơi pubg với liên quân mượt với giá như này thì quá tốt\n",
      "\n",
      "XLM Tokens:\n",
      "['<s>', '▁gói', '▁hàng', '▁cẩn', '▁thận', '▁', '.', '▁chơi', '▁pub', 'g', '▁với', '▁liên', '▁quân', '▁m', 'ượt', '▁với', '▁giá', '▁như', '▁này', '▁thì', '▁quá', '▁tốt', '</s>']\n",
      "\n",
      "PhoBERT Tokens (after NFC):\n",
      "['<s>', 'gói', 'hàng', 'cẩn', 'thận', '.', 'chơi', 'pub@@', 'g', 'với', 'liên', 'quân', 'mượt', 'với', 'giá', 'như', 'này', 'thì', 'quá', 'tốt', '</s>']\n"
     ]
    }
   ],
   "source": [
    "sample_text = train_texts_nfc[0]\n",
    "\n",
    "xlm_encoded = xlm_tokenizer(sample_text)\n",
    "phobert_encoded = phobert_tokenizer(sample_text)\n",
    "\n",
    "print(\"Text:\")\n",
    "print(sample_text)\n",
    "\n",
    "print(\"\\nXLM Tokens:\")\n",
    "print(xlm_tokenizer.convert_ids_to_tokens(xlm_encoded[\"input_ids\"]))\n",
    "\n",
    "print(\"\\nPhoBERT Tokens (after NFC):\")\n",
    "print(phobert_tokenizer.convert_ids_to_tokens(phobert_encoded[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f72a56-4d83-491d-aa2c-ab365e12be67",
   "metadata": {},
   "source": [
    "Do tính đa dạng và nhiễu của dữ liệu bình luận trực tuyến, chúng tôi lựa chọn tokenizer byte-level BPE của XLM-RoBERTa nhằm đảm bảo khả năng mã hóa ổn định và tránh hiện tượng từ ngoài từ điển."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3daaa7-fb38-45a1-9ded-b1ee67529092",
   "metadata": {},
   "source": [
    "## 2.5 Hàm tokenize + padding (dùng XLM-R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a4250-6f31-4346-9ad7-8c35c6cec588",
   "metadata": {},
   "source": [
    "Giá trị max_len được sử dụng để cố định độ dài chuỗi token nhằm tạo batch tensor đồng nhất, xây dựng attention mask và hỗ trợ positional encoding trong Transformer, đồng thời kiểm soát chi phí tính toán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cd1bcec-5fcc-4a77-aeb7-46d13ea78f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128 # Thêm max len để tạo các batch (tensor hình chữ nhật) + cố định chiều cho attention mask\n",
    "\n",
    "def tokenize_batch_xlm(texts, xlm_tokenizer, max_len):\n",
    "    return xlm_tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0448545-a60a-4510-8f17-0960e0f9fcce",
   "metadata": {},
   "source": [
    "## 2.6 Tokenize từng split (XLM-R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afada025-fa53-4fe0-98f5-9a12e3ac8909",
   "metadata": {},
   "source": [
    "Bước tokenize này chuyển mỗi câu văn bản thành chuỗi chỉ số có độ dài cố định, kèm theo attention mask để phân biệt token thật và padding, nhằm chuẩn bị đầu vào phù hợp cho Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa4ca94d-4a1a-41f4-8f81-443cdadf15db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[     0, 100929,   2508,  ...,      1,      1,      1],\n",
      "        [     0,   3087,  43561,  ...,      1,      1,      1],\n",
      "        [     0,   6184,  19474,  ...,      1,      1,      1],\n",
      "        ...,\n",
      "        [     0,      6,  11479,  ...,      1,      1,      1],\n",
      "        [     0,  11521,   5791,  ...,      1,      1,      1],\n",
      "        [     0,  10305,  29608,  ...,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n",
      "torch.Size([4387, 128])\n"
     ]
    }
   ],
   "source": [
    "xlm_train_encodings = tokenize_batch_xlm(train_texts, xlm_tokenizer, MAX_LEN)\n",
    "xlm_dev_encodings   = tokenize_batch_xlm(dev_texts, xlm_tokenizer, MAX_LEN)\n",
    "xlm_test_encodings  = tokenize_batch_xlm(test_texts, xlm_tokenizer, MAX_LEN)\n",
    "\n",
    "print(xlm_train_encodings.keys())\n",
    "print(xlm_train_encodings[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f200e592-463c-4180-b7ad-3bae8cdcf529",
   "metadata": {},
   "source": [
    "```yaml\n",
    "train_texts (list[str], length=4387)\n",
    "        ↓ tokenizer\n",
    "input_ids        : [4387, 128]  # token ids\n",
    "attention_mask   : [4387, 128]  # 0/1 mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835c6c2-89b7-443d-b001-71cc51c5ae54",
   "metadata": {},
   "source": [
    "## 2.7 Lấy nhãn domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cefb6688-f7b7-4092-9b23-79bb4d01a977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cosmetic', 'mobile', 'app', 'fashion'}\n"
     ]
    }
   ],
   "source": [
    "def collect_labels(data_dict):\n",
    "    return [v[\"domain\"] for v in data_dict.values()]\n",
    "\n",
    "train_labels = collect_labels(train_data)\n",
    "dev_labels   = collect_labels(dev_data)\n",
    "test_labels  = collect_labels(test_data)\n",
    "\n",
    "print(set(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69e530b-a928-4176-9695-90b7bd998cc3",
   "metadata": {},
   "source": [
    "## 2.8 Mapping label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2a863e1-5ae5-4f3a-a7c8-a8aeb52779c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app': 0, 'cosmetic': 1, 'fashion': 2, 'mobile': 3}\n"
     ]
    }
   ],
   "source": [
    "label2id = {label: idx for idx, label in enumerate(sorted(set(train_labels)))}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9de8ffb-38ed-48ba-8734-0a32d86bbdae",
   "metadata": {},
   "source": [
    "## 2.9 Encode label thành tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25acf3c2-7f03-476e-8fcd-827463a88e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4387])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def encode_labels(labels, label2id):\n",
    "    return torch.tensor([label2id[label] for label in labels])\n",
    "\n",
    "y_train = encode_labels(train_labels, label2id)\n",
    "y_dev   = encode_labels(dev_labels, label2id)\n",
    "y_test  = encode_labels(test_labels, label2id)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1eb10f4b-3576-400c-8649-cfec51594a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 3,  ..., 0, 0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad4c59-433b-48b3-86d3-c12424234a9b",
   "metadata": {},
   "source": [
    "```yaml\n",
    "train_texts (list[str], length=4387)\n",
    "        ↓ tokenizer\n",
    "input_ids        : [4387, 128]  # token ids\n",
    "attention_mask   : [4387, 128]  # 0/1 mask\n",
    "labels           : [4387]       # class ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97deb0-0803-4233-80be-3b8ffa621ca2",
   "metadata": {},
   "source": [
    "## 2.10 Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fdfd368-5b58-4847-858a-4b359b607a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([4387, 128])\n",
      "Attention mask shape: torch.Size([4387, 128])\n",
      "Labels shape: torch.Size([4387])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs shape:\", xlm_train_encodings[\"input_ids\"].shape)\n",
    "print(\"Attention mask shape:\", xlm_train_encodings[\"attention_mask\"].shape)\n",
    "print(\"Labels shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d15862e-33cb-4d16-923a-b31ebcaca8b5",
   "metadata": {},
   "source": [
    "## 2.11 some check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "915a251e-eb40-4e45-bd7c-1997a9eb3e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "gói hàng cẩn thận . chơi pubg với liên quân mượt với giá như này thì quá tốt\n",
      "\n",
      "Token IDs:\n",
      "tensor([     0, 100929,   2508,  24376,   5675,  12976,     19,      6,      5,\n",
      "         19702,  64792,    177, 137322,     14,   8151,  29225,    347,  11479,\n",
      "         12435,     18, 137322,     14,   3816,   1641,   1617,   2579,   6526,\n",
      "         29163,     18,      2,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n",
      "\n",
      "Attention mask:\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "Tokens:\n",
      "['<s>', '▁gói', '▁hàng', '▁cân', '▁th', 'ạ', 'n', '▁', '.', '▁chơi', '▁pub', 'g', '▁vơ', 'i', '▁liên', '▁quân', '▁m', 'ư', 'ơ', 't', '▁vơ', 'i', '▁giá', '▁như', '▁này', '▁thì', '▁quá', '▁tô', 't', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(train_texts[idx])\n",
    "\n",
    "print(\"\\nToken IDs:\")\n",
    "print(xlm_train_encodings[\"input_ids\"][idx])\n",
    "\n",
    "print(\"\\nAttention mask:\")\n",
    "print(xlm_train_encodings[\"attention_mask\"][idx])\n",
    "\n",
    "print(\"\\nTokens:\")\n",
    "print(xlm_tokenizer.convert_ids_to_tokens(\n",
    "    xlm_train_encodings[\"input_ids\"][idx]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88ece193-ddae-4e62-960b-89dc6a41fa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số padding token (5 câu đầu):\n",
      "tensor([ 98,   7,  52,  75, 109])\n"
     ]
    }
   ],
   "source": [
    "pad_id = xlm_tokenizer.pad_token_id\n",
    "\n",
    "num_pad = (xlm_train_encodings[\"input_ids\"] == pad_id).sum(dim=1)\n",
    "\n",
    "print(\"Số padding token (5 câu đầu):\")\n",
    "print(num_pad[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cabd5a5-f81c-46e5-828b-043e34f403a6",
   "metadata": {},
   "source": [
    "# 3. Lưu file pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2394603d-5b99-4bb9-a1ad-fb123ae83910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "processed_dir = \"./dataset/UIT-Vi-OCD/processed\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "def save_split(filename, encodings, labels):\n",
    "    path = os.path.join(processed_dir, filename)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"input_ids\": encodings[\"input_ids\"],\n",
    "            \"attention_mask\": encodings[\"attention_mask\"],\n",
    "            \"labels\": labels\n",
    "        },\n",
    "        path\n",
    "    )\n",
    "\n",
    "save_split(\"train.pt\", xlm_train_encodings, y_train)\n",
    "save_split(\"dev.pt\",   xlm_dev_encodings,   y_dev)\n",
    "save_split(\"test.pt\",  xlm_test_encodings,  y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1b182e0-2a33-41f5-b0a8-da73f3d39c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.load(\n",
    "    \"./dataset/UIT-Vi-OCD/processed/train.pt\"\n",
    ")\n",
    "\n",
    "print(train_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b511b5e-3e77-4d45-8da8-74e71205b25b",
   "metadata": {},
   "source": [
    "# 4. Post-tokenization EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbf4395f-b31b-4a79-a7ad-ca19b7338561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1bcc4a7-fcb2-47a4-ac01-7c0bde0e4149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 45.069068908691406 128\n"
     ]
    }
   ],
   "source": [
    "# Phân bố độ dài thật (không tính padding)\n",
    "\n",
    "input_ids = train_data[\"input_ids\"]\n",
    "attention_mask = train_data[\"attention_mask\"]\n",
    "\n",
    "real_lengths = attention_mask.sum(dim=1)\n",
    "\n",
    "print(\n",
    "    real_lengths.min().item(),\n",
    "    real_lengths.float().mean().item(),\n",
    "    real_lengths.max().item()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b66b79d-90e4-4b2b-99c8-490d612f9464",
   "metadata": {},
   "source": [
    "Min = 3 token | Max = 128 token | Mean = 45 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da06ab5b-8d97-46a1-ab7a-381712fe0a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6479)\n"
     ]
    }
   ],
   "source": [
    "# Tỷ lệ padding\n",
    "\n",
    "pad_ratio = 1 - real_lengths.float() / input_ids.size(1)\n",
    "print(pad_ratio.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd75ce32-866c-433d-804a-95c407da59cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1613, 1: 1043, 2: 1368, 3: 363}\n"
     ]
    }
   ],
   "source": [
    "# Phân bố nhãn\n",
    "labels = train_data[\"labels\"]\n",
    "\n",
    "unique, counts = torch.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique.tolist(), counts.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085806c-19c1-44af-9bf9-94884e9d91d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
